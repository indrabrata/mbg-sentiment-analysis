name: Data Collection

on:
  workflow_dispatch:
    inputs:
      keyword:
        description: "Search keyword"
        required: false
        default: "makan bergizi gratis"
  schedule:
    - cron: "0 18 * * *"

env:
  SEARCH_KEYWORD: ${{ github.event.inputs.keyword || vars.SEARCH_KEYWORD || 'makan bergizi gratis' }}
  TOTAL_DATA: ${{ vars.TOTAL_DATA || '50' }}

jobs:
  data-collection:
    runs-on: ubuntu-latest

    outputs:
      current_date: ${{ steps.date.outputs.current_date }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v2
        with:
          version: 8

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "pnpm"
          cache-dependency-path: "scrapper/pnpm-lock.yaml"

      - name: Install dependencies
        working-directory: ./scrapper
        run: pnpm install --no-frozen-lockfile

      - name: Build scrapper
        working-directory: ./scrapper
        run: pnpm build

      - name: Get yesterday's date in Indonesia timezone
        id: date
        run: |
          YESTERDAY_DATE=$(TZ='Asia/Jakarta' date -d 'yesterday' +%Y-%m-%d)
          echo "current_date=$YESTERDAY_DATE" >> $GITHUB_OUTPUT

      - name: Run scrapper
        working-directory: ./scrapper
        run: |
          node dist/bin.js \
            -o "mbg" \
            -s "${{ env.SEARCH_KEYWORD }} since:${{ steps.date.outputs.current_date }} lang:id" \
            --tab "LATEST" \
            -l ${{ env.TOTAL_DATA }} \
            --token ${{ secrets.TWITTER_TOKEN }}

      - name: Upload scraped RAW data
        uses: actions/upload-artifact@v4
        with:
          name: raw-${{ steps.date.outputs.current_date }}
          path: scrapper/tweets-data/mbg.csv

  data-preprocessing:
    needs: data-collection
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download RAW data
        uses: actions/download-artifact@v4
        with:
          name: raw-${{ needs.data-collection.outputs.current_date }}
          path: scrapper/tweets-data

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install uv
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"

      - name: Install Python dependencies
        working-directory: ./model
        run: |
          uv pip install --system -e .

      - name: Preprocess tweet data (cleaned)
        working-directory: ./model
        run: |
          python scripts/preprocess_data.py \
            --input ../scrapper/tweets-data/mbg.csv \
            --output_dir ../scrapper/tweets-data

      - name: Predict sentiment using ML model
        working-directory: ./scrapper/tweets-data
        run: |
          echo "ðŸ¤– Sending tweets to sentiment analysis API..."

          # Call the sentiment app API
          curl -X POST "https://mlops-app.cupcakez.my.id/predict-batch-tweet" \
            -H "Content-Type: multipart/form-data" \
            -F "file=@mbg_cleaned.csv" \
            -o "mbg_predictions_${{ needs.data-collection.outputs.current_date }}.csv" \
            --fail-with-body \
            --max-time 600

          echo "âœ… Sentiment predictions completed"

          # Show prediction summary
          if [ -f "mbg_predictions_${{ needs.data-collection.outputs.current_date }}.csv" ]; then
            echo "ðŸ“Š Prediction Statistics:"
            python3 -c "
          import pandas as pd
          df = pd.read_csv('mbg_predictions_${{ needs.data-collection.outputs.current_date }}.csv')
          print(f'Total predictions: {len(df)}')
          print(f'\\nSentiment distribution:')
          print(df['pred_label'].value_counts())
          print(f'\\nAverage confidence: {df[\"pred_score\"].mean():.4f}')
          "
          fi

      - name: Add empty label column (unlabeled dataset)
        working-directory: ./model
        run: |
          python scripts/add_label_column.py \
            --input ../scrapper/tweets-data/mbg_cleaned.csv \
            --output_dir ../scrapper/tweets-data \
            --date "${{ needs.data-collection.outputs.current_date }}"

      - name: Upload Cleaned Unlabeled data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unlabeled-${{ needs.data-collection.outputs.current_date }}
          path: scrapper/tweets-data/mbg_cleaned_unlabeled_${{ needs.data-collection.outputs.current_date }}.csv
